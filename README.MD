# [학습자료] Self Study AI (혼자 공부하는 머신러닝 + 딥러닝)

## 설명

- '혼자 공부하는 머신러닝 + 딥러닝'을 통해 학습한 자료입니다. 내용은 아는 것들이 많아서, 코드 위주로 정리했습니다. 그리고 이 페이지에 있는 설명들은 GPT-4에 의해 작성되었습니다. 기초 과정이 필요하다면 [이곳](https://github.com/foreverfl/-Book-Python-Data-Science-and-AI-Development)을 참조해주세요.

## 목차

- [Chapter01. 나의 첫 머신러닝](#Chapter01-나의-첫-머신러닝)
- [Chapter02. 데이터 다루기](#Chapter02-데이터-다루기)
- [Chapter03. 회귀 알고리즘과 모델 규제](#Chapter03-회귀-알고리즘과-모델-규제)
- [Chapter04. 다양한 분류 알고리즘](#Chapter04-다양한-분류-알고리즘)
- [Chapter05. 트리 알고리즘](#Chapter05-트리-알고리즘)
- [Chapter06. 비지도 학습](#Chapter06-비지도-학습)
- [Chapter07. 딥러닝을 시작합니다](#Chapter07-딥러닝을-시작합니다)
- [Chapter08. 이미지를 위한 인공 신경망](#Chapter08-이미지를-위한-인공-신경망)
- [Chapter09. 텍스트를 위한 인공 신경망](#Chapter09-텍스트를-위한-인공-신경망)

## Chapter01. 나의 첫 머신러닝

## Chapter02. 데이터 다루기

## Chapter03. 회귀 알고리즘과 모델 규제

## Chapter04. 다양한 분류 알고리즘

## Chapter05. 트리 알고리즘

## Chapter06. 비지도 학습

## Chapter07. 딥러닝을 시작합니다

### batch_size

> - **개념**: 각각의 배치에서 모델에게 주입할 샘플의 개수. 예를 들어, 데이터셋에 총 1000개의 샘플이 있고 batch_size가 32라면, 각 배치에는 32개의 샘플이 포함될 것이고, 이를 통해 모델이 업데이트 됨. 총 배치의 개수는 1000 / 32 = 31 (나머지 8개는 마지막 배치에 포함됨)가 됨.
> - **작은 batch_size**
>   > - **메모리 사용량**: 작은 배치 크기는 적은 메모리를 사용하므로, 작은 하드웨어에서도 학습이 가능.
>   > - **학습 속도**: 적은 양의 데이터로 더 자주 모델이 업데이트되므로, 학습 속도가 빠를 수 있음. 하지만 이는 경우에 따라 다름.
>   > - **정확도**: 더 자주 업데이트되므로, 좀 더 세밀한 모델 튜닝이 가능. 하지만, 이것이 항상 더 나은 성능을 보장하는 것은 아님.
>   > - **일반화**: 작은 배치 크기는 모델이 훈련 데이터에 덜 "적응"되게 하므로 일반화 성능이 더 좋을 수 있음.
> - **큰 batch_size**
>   > - **메모리 사용량**: 큰 배치 크기는 더 많은 메모리를 사용.
>   > - **학습 속도**: 한 번의 업데이트에 더 많은 시간이 걸리므로, 학습이 느릴 수 있음.
>   > - **정확도**: 큰 배치로 인해 덜 자주 업데이트되므로, 모델의 성능이 낮아질 수 있음.
>   > - **수렴**: 큰 배치 크기는 수렴을 더 빨리 하게 할 수 있지만, 이는 최적의 솔루션을 찾지 못할 수도 있음.
> - **기본값**: Keras에서 기본 batch_size를 32로 설정한 것은 "최적의 값"이라고 할 수는 없음. 사실, "최적의 배치 크기"는 문제에 따라 다르고, 하드웨어 리소스, 메모리 사용량, 학습 속도 등 다양한 요인에 의존. 그럼에도 불구하고, 32는 실제로 많은 연구와 경험에서 잘 작동하는 값으로 알려져 있음.

### 옵티마이저

> - **개념**: 옵티마이저(optimizer)란, 딥러닝 모델을 학습할 때 가중치를 어떻게 업데이트할지 결정하는 알고리즘. 다양한 옵티마이저가 있고, 각각의 특성과 장단점이 있음.
> - **SGD (Stochastic Gradient Descent)**
>   > - **개념**: 기본적인 경사 하강법. 각 스텝에서 일부 데이터만을 사용해 가중치를 업데이트.
>   > - **장점**: 단순하고 계산이 빠름.
>   > - **단점**: 학습이 느리거나 지역 최적점에 갇힐 가능성이 있음.
>   > - **일반적인 사용 상황**: 데이터셋이 작고, 문제가 단순할 때 유용함.
> - **Momentum**
>   > - **개념**: 이전의 그래디언트를 일정 부분 유지하면서 가중치를 업데이트.
>   > - **장점**: SGD의 단점을 어느 정도 해결하며, 보다 빠르고 안정적인 학습이 가능.
>   > - **단점**: 하이퍼파라미터(모멘텀 계수)를 추가로 설정해야 함.
>   > - **일반적인 사용 상황**: SGD의 느린 수렴을 해결하고자 할 때, 또는 지역 최적점(local minima)에 빠지는 것을 피하고자 할 때.
> - **Nesterov Momentum**
>   > - **개념**: Momentum의 변형으로, 현재 위치가 아닌 업데이트될 위치에서의 그래디언트를 사용.
>   > - **장점**: Momentum보다 더 빠른 수렴이 가능.
>   > - **단점**: Momentum의 단점과 동일.
>   > - **일반적인 사용 상황**: Momentum을 사용할 때보다 더 빠르게 수렴을 원하거나, 최적화 과정에서의 오실레이션을 줄이고자 할 때.
> - **RMSprop**
>   > - **개념**: 각 파라미터에 대해 학습률을 동적으로 조정.
>   > - **장점**: 불규칙한 손실 함수에서도 잘 작동.
>   > - **단점**: 하이퍼파라미터가 추가됨.
>   > - **일반적인 사용 상황**: 비정형이거나 노이즈가 많은 데이터셋에서 유용함. RNN과 같은 순환 신경망에도 종종 사용됨.
> - **Adam**
>   > - **개념**: Momentum과 RMSprop의 아이디어를 결합한 옵티마이저.
>   > - **장점**: 다양한 문제에 대해 빠르고 안정적인 학습이 가능.
>   > - **단점**: 하이퍼파라미터가 더 추가됨.
>   > - **일반적인 사용 상황**: 여러 종류의 문제에 대해 잘 작동하는 '만능' 옵티마이저로 광범위하게 사용됨. 특히 크고 복잡한 데이터셋에 유용함.
> - **Adagrad**
>   > - **개념**: 많이 업데이트되는 파라미터는 학습률을 빠르게 감소시킴.
>   > - **장점**: 희소한 데이터에 잘 작동할 수 있음.
>   > - **단점**: 학습이 너무 빨리 느려질 수 있음.
>   > - **일반적인 사용 상황**: 희소한 데이터셋에서 유용하며, 텍스트 데이터 같은 고차원이고 희소한 데이터에 자주 사용됨.

### 지역최적점(Local Minima)와 오실레이션(Oscillation)

> - **지역 최적점 (Local Minima)**: 지역 최적점은 그 주변에서 가장 낮은 값이지만, 전체 데이터에 대해서는 반드시 최저점이라고 할 수 없는 지점. 즉, 근처에만 놓고 보면 가장 좋은 해답 같아 보이지만, 더 넓은 범위에서 보면 더 좋은 해답이 있을 수 있음. 예를 들어, 산에서 등산을 한다고 생각해보면, 지역 최적점은 작은 골짜기와 같은 곳으로, 주변보다는 낮지만 아직 산의 정상이나 바닷가까지 가지는 않은 상태.
> - **오실레이션 (Oscillation)**: 오실레이션은 주기적으로 왔다갔다 하는 현상. 최적화에서 오실레이션은 파라미터가 빠르게 변동하면서 최적점을 찾지 못하고 왔다갔다하는 현상을 의미. 이것을 높은 언덕과 깊은 골짜기가 연속으로 나타나는 산에서 등산을 하다가 너무 빨리 움직여서 계속 앞으로 가다가 뒤로 가다가를 반복하는 것으로 상상하면 됨.

## Chapter08. 이미지를 위한 인공 신경망

### 패딩(Padding)

> - **세임 패딩(same padding)**
>   > - 입력과 출력의 차원(높이와 너비)이 동일하도록 하는 패딩 방식.
>   > - 입력 주변에 0을 추가하여 합성곱 연산을 진행.
>   > - 주로 패딩의 크기는 (필터의 크기 - 1) / 2로 설정됨.
> - **밸리드 패딩(valid padding)**
>   > - 패딩을 적용하지 않고 합성곱 연산을 수행함.
>   > - 이 경우, 출력의 차원이 입력보다 작아짐.

### 풀링(Pooling)

> - **최대 풀링(max pooling)**
>   > - 지정한 영역에서 최대값을 선택하여 차원을 축소함.
>   > - 일반적으로 2x2 크기의 필터와 스트라이드 2를 사용하여 차원을 절반으로 줄임.
>   > - 최대값이 중요한 특징을 가지고 있는 경우나 불변성(invariance)을 높여야 하는 경우 (예: 회전, 이동 등에 불변인 특성 추출)에 사용함.
> - **평균 풀링(average pooling)**
>   > - 지정한 영역의 평균값을 사용하여 차원을 축소.
>   > - 최대 풀링과 마찬가지로 2x2 크기의 필터와 스트라이드 2를 자주 사용.
>   > - 특징이 균일하게 분포되어 있는 경우나 최대값만을 고려하기에는 정보 손실이 발생할 가능성이 있는 경우에 사용함.

## Chapter09. 텍스트를 위한 인공 신경망
